{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e6b951-36e9-45ba-9403-38f2f68f6458",
   "metadata": {},
   "source": [
    "# Hands-On Week 1\n",
    "In week 1 we've learned about calculating the information in spoken language. In other words, how can we go from an audio signal to strings of text, to measures of information transfer. <br>\n",
    "In this first hands-on class, we will take a closer look at quantifiyng information in language. We'll learn how to use Python to train a very simple model of next-word probabilities, and how we can use these values to learn about spoken language use. We provide you with the main components, but you will need to use your own python skills to complete exercise!<br>\n",
    "Before we begin, download the DailyDialog corpus from Kaggle: https://www.kaggle.com/datasets/thedevastator/dailydialog-unlock-the-conversation-potential-in?resource=download "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50709fc9-f0d7-40f3-b9b5-1f90c0f42889",
   "metadata": {},
   "source": [
    "## Initializing Variables\n",
    "First, we will need to initialize some variables and load in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "617647c9-cbee-4076-b819-c5d9e49e8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk import trigrams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "####\n",
    "# Set up NLP parameters\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Create a placeholder for probability model\n",
    "model_prob = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "# Create a placeholder for surprisal model\n",
    "model_surprisal = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# update as the model is trained\n",
    "max_surprisal = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79adc171-f30d-49a6-989a-0dd2bc24e27a",
   "metadata": {},
   "source": [
    "### DIY: Preparing the corpus\n",
    "Now that the packages are loaded, we need to read in the corpus, and format it for processing. Our simple language model requires us to have a <i>list</i> of strings, where each string is a sentence in the corpus. The corpus that we are using, the MultiDialogue Corpus, is not yet in this format. This is because the corpus has more features than we actually need. It provides a list of dialogues (collections of exchanges), that each consist of several speech turns (i.e., sentences, for the purpose of this tutorial), as well emotion and dialogue act labels. <br>\n",
    "<b>Your Task:</b><br>\n",
    "- Inspect the format of the data\n",
    "- Extract the sentences, removing any punctuation and removing contractions (hint: check out the contractions and string packages)\n",
    "- Create a new list variable called 'sentences' that contains only the speech sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af55a723-2f4f-4b50-be46-6070b8c8ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Say  Jim  how about going for a few beers after dinner  ', '  You know that is tempting but is really not good for our fitness  ', '  What do you mean  It will help us to relax  ', '  Do you really think so  I do not  It will just make us fat and act silly  Remember last time  ', '  I guess you are rightBut what shall we do  I do not feel like sitting at home  ', '  I suggest a walk over to the gym where we can play singsong and meet some of our friends  ', '  That is a good idea  I hear Mary and Sally often go there to play pingpongPerhaps we can make a foursome with them  ', '  Sounds great to me  If they are willing  we could ask them to go dancing with usThat is excellent exercise and fun  too  ', '  GoodLet  s go now    All right  ', 'Can you do pushups  ']\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = pd.read_csv(\"./archive/train.csv\")\n",
    "raw_corpus.head()\n",
    "\n",
    "\n",
    "# REMOVE\n",
    "import contractions\n",
    "import string\n",
    "sentences = []\n",
    "for idx, row in raw_corpus.iterrows():\n",
    "    for sentence in row['dialog'].split('\\n'):\n",
    "        sentence_cleaned = contractions.fix(sentence)\n",
    "        sentence_cleaned = sentence_cleaned.translate(str.maketrans('', '', ','))\n",
    "\n",
    "        sentence_cleaned = sentence_cleaned.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "        sentences.append(sentence_cleaned)\n",
    "\n",
    "print(sentences[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770ec35-1189-4d4f-8f88-f62730428282",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287014f-7d2b-488e-bac9-11616275f2eb",
   "metadata": {},
   "source": [
    "## Calculating Surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4b09a7b-8618-468d-957d-49467c9ef8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Count frequency of co-occurance\n",
    "for sent_num in range(len(sentences)):\n",
    "    sentence = sentences[sent_num]\n",
    "    sequence= sentence.split(\" \")\n",
    "    for w1, w2, w3 in trigrams(sequence, pad_right=True, pad_left=True):\n",
    "        model_prob[(w1, w2)][w3] += 1\n",
    "        model_surprisal[(w1, w2)][w3] += 1\n",
    "\n",
    "#Step 5: Transform the counts to probabilities\n",
    "for w1_w2 in model_prob:\n",
    "    total_count = float(sum(model_prob[w1_w2].values()))\n",
    "    for w3 in model_prob[w1_w2]:\n",
    "        model_prob[w1_w2][w3] /= total_count # probability\n",
    "\n",
    "#Step 6: Transform the counts to surprisal\n",
    "for w1_w2 in model_surprisal:\n",
    "    total_count = float(sum(model_surprisal[w1_w2].values()))\n",
    "    for w3 in model_surprisal[w1_w2]:\n",
    "        if w3:\n",
    "            probability = model_surprisal[w1_w2][w3] / total_count  \n",
    "            \n",
    "            model_surprisal[w1_w2][w3] = -math.log(probability) # <-- Smith and Levy, 2013\n",
    "            \n",
    "            # update max -- this is used for corpus values not encountered during training\n",
    "            if -math.log(probability) > max_surprisal:\n",
    "                max_surprisal = -math.log(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d054122b-af56-49a6-8404-87ec270aba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00046446818392940084\n",
      "0.08732001857872736\n"
     ]
    }
   ],
   "source": [
    "#Step 7: Test probability model\n",
    "print(model_prob[('I','think')]['sometimes'])\n",
    "print(model_prob[('I','think')]['that'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8916b049-f227-4a74-b5c4-4879c2b23662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.640282587785199\n",
      "7.157735484249907\n"
     ]
    }
   ],
   "source": [
    "#Step 8: Test surprisal model\n",
    "print(model_surprisal['are','you']['going'])\n",
    "print(model_surprisal['are','you']['happy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4257a8-015a-4734-84c7-998d9d02ea19",
   "metadata": {},
   "source": [
    "## Step 7: Next-Word Entropy\n",
    "While surprisal captures the new information gained at each word, next-word entropy measures how uncertain you are about the next upcoming word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5571230-5740-4a79-8e0c-805f2c95120c",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def calc_entropy(w1_w2,model_surprisal):\n",
    "    # expects w1_w2 to be of the format: [\"some\",\"words\"]\n",
    "    prob_dist = [ model_surprisal[w1_w2][val3] for val3 in model_surprisal[w1_w2]]\n",
    "    \n",
    "    NWE = entropy(prob_dist, base=2)\n",
    "    return NWE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d19595-96b2-458f-acfc-3b61e852abcb",
   "metadata": {},
   "source": [
    "### DIY: \n",
    "- Read in the train.csv file\n",
    "- Use your trained model to calculate, and visualize, the surprisal or NWE profile within dialogues or sentences\n",
    "- <b>Answer</b> in the LC1 review: what patterns do you see?\n",
    "- Calculate average surprisal per dialogue act <b>or</b> per emotion category\n",
    "- <b>Answer</b> in the LC1 review: do you see any correlations between surprisal/NWE and a particular dialogue act or emotion?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
